\documentclass{article}

\usepackage{neurips_2026}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{See, Then Think: Two-Phase VLM Training for Game Understanding}

\author{
  David Zeng \\
  % TODO: Add affiliation \\
  % TODO: Add email \\
}

\begin{document}

\maketitle

\begin{abstract}
Vision-language models (VLMs) struggle with domain-specific visual grounding tasks in competitive gaming environments, where rapid and accurate scene understanding is critical for strategic decision-making. We propose a two-phase training paradigm: first, supervised fine-tuning on screenshot--game state pairs teaches the model accurate visual grounding (reading the HUD); then, Group Relative Policy Optimization with demo-derived rewards teaches strategic reasoning scored against professional play and round outcomes. We evaluate our approach on Counter-Strike 2 (CS2) screenshot analysis tasks, demonstrating that separating perception and reasoning into distinct training phases outperforms single-phase approaches.
\end{abstract}

\section{Introduction}
\label{sec:intro}

% TODO: Introduce the problem of building AI gaming copilots
% TODO: Motivate why VLMs are a natural fit for this domain
% TODO: Explain the challenge: VLMs trained on general data struggle with domain-specific visual grounding
% TODO: Present the key hypothesis: separating perception (SFT on screenshots) from reasoning (GRPO on demo rewards) is more effective than training both jointly
% TODO: Summarize contributions and results

\section{Related Work}
\label{sec:related}

% TODO: Cover VLM architectures and training paradigms (Qwen-VL, LLaVA, etc.)
% TODO: Discuss parameter-efficient fine-tuning methods (LoRA)
% TODO: Review reinforcement learning from human feedback and GRPO
% TODO: Survey game AI and esports analytics work
% TODO: Position our work relative to existing approaches

\section{Method}
\label{sec:method}

% TODO: Overview of the Chimera system architecture
% TODO: Explain the two-stage training pipeline

\subsection{Problem Formulation}
\label{sec:problem}

% TODO: Formalize the visual grounding task for CS2
% TODO: Define input/output spaces (screenshots -> strategic advice)
% TODO: Describe the structured replay data format

\subsection{Phase 1: Visual Grounding via SFT}
\label{sec:sft}

% TODO: Explain Phase 1: SFT on screenshot--game state pairs from demo-synchronized VOD frames
% TODO: Describe the dataset: screenshots paired with engine-accurate ground truth from demo files
% TODO: Detail the LoRA configuration — vision + language layers both trainable
% TODO: Discuss how demo data provides exact game state (health, armor, weapons, player counts) without model labeling

\subsection{Phase 2: Strategic Reasoning via GRPO}
\label{sec:grpo}

% TODO: Explain Phase 2: GRPO with 3 reward signals + multiplicative format gate (D013)
% TODO: R_percept (0.20) prevents SFT regression; vision layers frozen
% TODO: R_decision (0.30) — behavioral feature alignment with pro play from tick data
% TODO: R_outcome (0.50) — outcome-modulated decision reward with player contribution φ
% TODO: KL regularization (λ=0.02) prevents mode collapse
% TODO: Describe outcome reward asymmetry: deviating from winning play not penalized hard, endorsing losing play is

\subsection{Reward Design}
\label{sec:reward}

% TODO: Detail the 3 reward functions + multiplicative format gate (D013)
%   Format gate: multiplicative mask — invalid JSON → zero total reward
%   R_percept (α=0.20): merged hard+soft field accuracy (prevents SFT regression)
%   R_decision (β=0.30): behavioral feature alignment (d_move, d_obj, u_type, e_timing, δ_engage)
%   R_outcome (γ=0.50): R_decision · Ω(W, φ, a) — outcome-modulated with player contribution
%   KL penalty (λ=0.02): regularization against SFT reference
% TODO: Explain behavioral feature vectors extracted from tick data
% TODO: Describe Ω function and signal matrix (agree+win, agree+lose, deviate+win, deviate+lose)
% TODO: Explain player contribution φ for credit assignment
% TODO: Address the challenge of evaluating open-ended strategic suggestions
% TODO: Discuss mechanical skill (aim) as a latent confound in R_outcome (D015)
%   - φ attenuates outcome signal when mechanics dominate (good decision + bad aim → low φ → damped)
%   - R_decision provides aim-independent 30% signal (strategic features orthogonal to execution)
%   - Population averaging over multiple pros washes out mechanically-dependent plays
%   - Vulnerability: bad decision + good aim → high φ → amplified bad signal; mitigated by population averaging
%   - The model learns E[outcome | decision, game_state] averaged over pro-level aim distribution

\begin{figure}[t]
\centering
\fbox{\parbox{0.9\linewidth}{\centering\vspace{2in}[Placeholder: System Architecture Diagram]}}
\caption{Overview of the Chimera training pipeline. Phase 1 performs SFT on screenshot--game state pairs to learn visual grounding (reading the HUD). Phase 2 applies GRPO with 3 reward signals gated by a multiplicative format constraint: $R_\text{percept}$ (perceptual accuracy), $R_\text{decision}$ (behavioral alignment with pro play), and $R_\text{outcome}$ (outcome-modulated decision reward with player contribution weighting). Vision layers are frozen in Phase 2; KL regularization prevents mode collapse.}
\label{fig:architecture}
\end{figure}

\section{Experiments}
\label{sec:experiments}

% TODO: Describe the overall experimental setup and evaluation protocol

\subsection{Setup}
\label{sec:setup}

% TODO: Detail the dataset (size, sources, split)
% Base model: Qwen3.5-27B (dense, 27B params). Selected over Qwen3.5-35B-A3B (MoE, 35B total / 3B active)
% because MoE's packed expert tensors are incompatible with BnB 4-bit quantization on consumer GPUs (D019).
% We use a pre-quantized NF4 checkpoint (skkwowee/Qwen3.5-27B-bnb-4bit) for reproducible loading.
% Dense model loads cleanly with QLoRA on a single RTX 4090 (24GB).
% Training: QLoRA via peft + bitsandbytes (LoRA rank r=16-64 on NF4 base). Both SFT and GRPO train only LoRA adapters
% (~0.1-1% of parameters); base model weights are frozen. Vision layers trainable in SFT, frozen in GRPO (D007).
% TODO: Specify training hyperparameters for both phases
% TODO: Define evaluation metrics

\subsection{Baselines}
\label{sec:baselines}

% TODO: Describe baseline approaches:
%   - Model A: Zero-shot VLM (no training)
%   - Model B: SFT only (visual grounding, no GRPO)
%   - Model C: SFT + GRPO (full two-phase pipeline)
%   - Model D: GRPO only (no SFT, tests whether SFT phase is necessary)
% TODO: Explain comparison methodology

\subsection{Results}
\label{sec:results}

% TODO: Present quantitative results comparing two-phase (SFT+GRPO) vs single-phase
% TODO: Show ablation studies (SFT-only vs GRPO-only vs both, reward weight sensitivity)
% TODO: Include qualitative examples of generated advice

\begin{table}[t]
\centering
\caption{Performance comparison on CS2 screenshot analysis. Two-phase training (SFT + GRPO) outperforms single-phase approaches across all metrics.}
\label{tab:results}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & $\mathbf{R_\text{percept}}$ & $\mathbf{R_\text{decision}}$ & $\mathbf{R_\text{outcome}}$ & \textbf{Weighted Total} \\
\midrule
Zero-shot (A) & -- & -- & -- & -- \\
SFT only (B) & -- & -- & -- & -- \\
SFT + GRPO (C, Ours) & -- & -- & -- & -- \\
GRPO only (D) & -- & -- & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

\section{Analysis}
\label{sec:analysis}

% TODO: Analyze why separating perception and reasoning into two phases works
% TODO: Investigate what the model learns in each phase (SFT: HUD reading; GRPO: decision-making)
% TODO: Discuss failure cases and limitations
% TODO: Mechanical skill confound analysis (D015):
%   - Round outcome = f(decision_quality, mechanical_skill, teammate, opponent, randomness)
%   - Architecture handles via 3 layers: φ attenuation, aim-independent R_decision, population averaging
%   - Limitation: small dataset (4 demos, ~3000 samples) limits statistical power for noise averaging
%   - Limitation: φ conflates aim and decisions (damage correlates with both)
%   - The "donk problem": unconventional plays decompose into superior game reading (learnable) vs purely mechanical (noise)
%   - Model learns expected value at pro skill level, not universal strategic truth
%   - Primary mitigation is more diverse demos, not architectural change
% TODO: Explore computational efficiency gains from freezing vision layers during GRPO

\section{Conclusion}
\label{sec:conclusion}

% TODO: Summarize the key findings
% TODO: Reiterate the benefits of two-phase training (perception then reasoning)
% TODO: Discuss broader implications: the paradigm applies wherever cheap structured data exists alongside expensive visual data (robotics, medical imaging, autonomous driving)
% TODO: Suggest future directions (other games, multimodal reasoning tasks)

\bibliographystyle{plain}
\bibliography{references}

\end{document}
