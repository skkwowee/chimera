\documentclass{article}

\usepackage{neurips_2026}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{See, Then Think: Two-Phase VLM Training for Game Understanding}

\author{
  David Zeng \\
  % TODO: Add affiliation \\
  % TODO: Add email \\
}

\begin{document}

\maketitle

\begin{abstract}
Vision-language models (VLMs) struggle with domain-specific visual grounding tasks in competitive gaming environments, where rapid and accurate scene understanding is critical for strategic decision-making. We propose a two-phase training paradigm: first, supervised fine-tuning on screenshot--game state pairs teaches the model accurate visual grounding (reading the HUD); then, Group Relative Policy Optimization with demo-derived rewards teaches strategic reasoning scored against professional play and round outcomes. We evaluate our approach on Counter-Strike 2 (CS2) screenshot analysis tasks, demonstrating that separating perception and reasoning into distinct training phases outperforms single-phase approaches.
\end{abstract}

\section{Introduction}
\label{sec:intro}

% TODO: Introduce the problem of building AI gaming copilots
% TODO: Motivate why VLMs are a natural fit for this domain
% TODO: Explain the challenge: VLMs trained on general data struggle with domain-specific visual grounding
% TODO: Present the key hypothesis: separating perception (SFT on screenshots) from reasoning (GRPO on demo rewards) is more effective than training both jointly
% TODO: Summarize contributions and results

\section{Related Work}
\label{sec:related}

% TODO: Cover VLM architectures and training paradigms (Qwen-VL, LLaVA, etc.)
% TODO: Discuss parameter-efficient fine-tuning methods (LoRA)
% TODO: Review reinforcement learning from human feedback and GRPO
% TODO: Survey game AI and esports analytics work
% TODO: Position our work relative to existing approaches

\section{Method}
\label{sec:method}

% TODO: Overview of the Chimera system architecture
% TODO: Explain the two-stage training pipeline

\subsection{Problem Formulation}
\label{sec:problem}

% TODO: Formalize the visual grounding task for CS2
% TODO: Define input/output spaces (screenshots -> strategic advice)
% TODO: Describe the structured replay data format

\subsection{Phase 1: Visual Grounding via SFT}
\label{sec:sft}

% TODO: Explain Phase 1: SFT on screenshot--game state pairs from demo-synchronized VOD frames
% TODO: Describe the dataset: screenshots paired with engine-accurate ground truth from demo files
% TODO: Detail the LoRA configuration â€” vision + language layers both trainable
% TODO: Discuss how demo data provides exact game state (health, armor, weapons, player counts) without model labeling

\subsection{Phase 2: Strategic Reasoning via GRPO}
\label{sec:grpo}

% TODO: Explain Phase 2: GRPO with 7 reward signals derived from pro demo data
% TODO: Vision rewards (format gate, hard/soft field accuracy) prevent SFT regression; vision layers frozen
% TODO: Reasoning rewards (decision alignment vs pro play, outcome weighting, consistency, reasoning quality) are the RL training signal
% TODO: Describe outcome reward asymmetry: deviating from winning play not penalized hard, endorsing losing play is

\subsection{Reward Design}
\label{sec:reward}

% TODO: Detail the 7 reward functions and their weights
%   Vision (prevent SFT regression): format gate (0.05), hard field accuracy (0.15), soft field accuracy (0.05)
%   Reasoning (RL signal): decision alignment (0.15), outcome reward (0.30), consistency (0.20), reasoning quality (0.10)
% TODO: Explain action taxonomy for categorizing model advice vs pro play
% TODO: Describe outcome reward signal matrix (agree+win, agree+lose, deviate+win, deviate+lose)
% TODO: Address the challenge of evaluating open-ended strategic suggestions

\begin{figure}[t]
\centering
\fbox{\parbox{0.9\linewidth}{\centering\vspace{2in}[Placeholder: System Architecture Diagram]}}
\caption{Overview of the Chimera training pipeline. Phase 1 performs SFT on screenshot--game state pairs to learn visual grounding (reading the HUD). Phase 2 applies GRPO with demo-derived rewards to learn strategic reasoning (decision alignment with pro play, outcome weighting). Vision layers are frozen in Phase 2.}
\label{fig:architecture}
\end{figure}

\section{Experiments}
\label{sec:experiments}

% TODO: Describe the overall experimental setup and evaluation protocol

\subsection{Setup}
\label{sec:setup}

% TODO: Detail the dataset (size, sources, split)
% TODO: Describe the base model (Qwen2-VL)
% TODO: Specify training hyperparameters for both phases
% TODO: Define evaluation metrics

\subsection{Baselines}
\label{sec:baselines}

% TODO: Describe baseline approaches:
%   - Model A: Zero-shot VLM (no training)
%   - Model B: SFT only (visual grounding, no GRPO)
%   - Model C: SFT + GRPO (full two-phase pipeline)
%   - Model D: GRPO only (no SFT, tests whether SFT phase is necessary)
% TODO: Explain comparison methodology

\subsection{Results}
\label{sec:results}

% TODO: Present quantitative results comparing two-phase (SFT+GRPO) vs single-phase
% TODO: Show ablation studies (SFT-only vs GRPO-only vs both, reward weight sensitivity)
% TODO: Include qualitative examples of generated advice

\begin{table}[t]
\centering
\caption{Performance comparison on CS2 screenshot analysis. Two-phase training (SFT + GRPO) outperforms single-phase approaches across all metrics.}
\label{tab:results}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Field Acc.} & \textbf{Decision Align.} & \textbf{Consistency} & \textbf{Weighted Total} \\
\midrule
Zero-shot (A) & -- & -- & -- & -- \\
SFT only (B) & -- & -- & -- & -- \\
SFT + GRPO (C, Ours) & -- & -- & -- & -- \\
GRPO only (D) & -- & -- & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

\section{Analysis}
\label{sec:analysis}

% TODO: Analyze why separating perception and reasoning into two phases works
% TODO: Investigate what the model learns in each phase (SFT: HUD reading; GRPO: decision-making)
% TODO: Discuss failure cases and limitations
% TODO: Explore computational efficiency gains from freezing vision layers during GRPO

\section{Conclusion}
\label{sec:conclusion}

% TODO: Summarize the key findings
% TODO: Reiterate the benefits of two-phase training (perception then reasoning)
% TODO: Discuss broader implications: the paradigm applies wherever cheap structured data exists alongside expensive visual data (robotics, medical imaging, autonomous driving)
% TODO: Suggest future directions (other games, multimodal reasoning tasks)

\bibliographystyle{plain}
\bibliography{references}

\end{document}
