# Chimera Configuration

# HuggingFace Hub
hub:
  dataset_repo: "skkwowee/chimera-cs2"
  model_repo: "skkwowee/chimera-cs2-qwen3-vl-8b"

# Paths
paths:
  raw_screenshots: "data/raw"
  labeled_data: "data/labeled"
  processed_data: "data/processed"

# Claude API settings (for labeling)
claude:
  model: "claude-opus-4-6"
  max_tokens: 1024

# Local VLM settings (for inference)
vlm:
  model_name: "Qwen/Qwen3-VL-8B-Instruct"
  device: "cuda"
  torch_dtype: "bfloat16"

# CS2 game state schema
game_state:
  fields:
    - map_name
    - round_phase        # buy, playing, post-plant
    - player_side        # T or CT
    - player_health
    - player_armor
    - player_money
    - team_money_total
    - weapon_primary
    - weapon_secondary
    - utility            # list of grenades
    - alive_teammates
    - alive_enemies
    - bomb_status        # carried, planted, defused, exploded, null
    - site               # A, B, mid, etc.
    - visible_enemies    # count

# Labeling prompt template
labeling:
  system_prompt: |
    You are an expert CS2 analyst. Given a screenshot from Counter-Strike 2,
    extract the game state and provide strategic advice.

    Be precise about numbers (health, money, ammo) and positions.
    For strategic advice, consider economy, positioning, and round context.

# SFT Training settings (run before GRPO)
sft_training:
  model_name: "Qwen/Qwen3-VL-8B-Instruct"
  use_4bit: true
  torch_dtype: "bfloat16"

  # LoRA settings
  use_lora: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05

  # Training hyperparameters
  num_epochs: 3
  batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 2e-5
  warmup_ratio: 0.1

  # SFT-specific
  max_seq_length: 2048
  finetune_vision_layers: true  # SFT trains vision; GRPO freezes for vLLM

  # Output (merged model used as GRPO base)
  output_dir: "outputs/sft"
  save_steps: 100
  logging_steps: 10

# GRPO Training settings
training:
  # Model settings
  model_name: "Qwen/Qwen3-VL-8B-Instruct"
  use_4bit: true
  use_vllm: true
  torch_dtype: "bfloat16"

  # LoRA settings
  use_lora: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05

  # Training hyperparameters
  num_epochs: 3
  batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 2e-5
  warmup_ratio: 0.1

  # GRPO settings
  num_generations: 4
  max_new_tokens: 1024
  temperature: 0.7
  importance_sampling_level: "sequence"  # GSPO variant

  # Reward weights: R_percept, R_decision, R_outcome (D013)
  reward_weights: [0.20, 0.30, 0.50]

  # Output
  output_dir: "outputs/grpo"
  save_steps: 100
  logging_steps: 10
